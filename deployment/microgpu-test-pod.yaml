apiVersion: v1
kind: Pod
metadata:
  name: microgpu-test-pod
spec:
  restartPolicy: OnFailure
  containers:
    - name: qwen-inference
      image: nvcr.io/nvidia/pytorch:24.05-py3
      command: ["/bin/bash", "-c"]
      args:
        - >
          pip install transformers torch accelerate -i https://pypi.tuna.tsinghua.edu.cn/simple &&
          python -c "
          from transformers import AutoModelForCausalLM, AutoTokenizer
          import torch;
          device = 'cuda:0' if torch.cuda.is_available() else 'cpu';
          print(f'Using device: {device}');
          model = AutoModelForCausalLM.from_pretrained('/model', torch_dtype=torch.float16, device_map=device);
          tokenizer = AutoTokenizer.from_pretrained('/model');
          inputs = tokenizer('你好，请介绍一下你自己', return_tensors='pt').to(device);
          outputs = model.generate(**inputs, max_new_tokens=50);
          print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      resources:
        requests:
          nvidia.com/microgpu: 1  # 请求自定义GPU资源
        limits:
          nvidia.com/microgpu: 1
      volumeMounts:
        - name: model-storage
          mountPath: /model
        - name: cuda-lib
          mountPath: /usr/local/nvidia/lib64 # 容器内标准路径
  volumes:
    - name: model-storage
      hostPath:
        path: /home/k8sadmin/Qwen1.5-0.5B-Chat
        type: Directory
    - name: cuda-lib
      hostPath:
        path: /usr/lib/x86_64-linux-gnu