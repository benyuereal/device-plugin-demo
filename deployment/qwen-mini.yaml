# qwen-gpu-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen-mini
spec:
  replicas: 1
  selector:
    matchLabels:
      app: qwen
  strategy:
    type: Recreate  # 确保单副本模式安全更新
  template:
    metadata:
      labels:
        app: qwen
    spec:
      # 添加节点选择器，确保调度到有模型的节点
      nodeName: minikube  # 替换为您的实际节点名
      
      containers:
        - name: qwen-inference
          image: nvcr.io/nvidia/pytorch:24.05-py3  # 官方支持CUDA的镜像
          command: ["/bin/sh", "-c"]
          args:
            - |
              # 安装依赖
              pip install transformers==4.40.0 accelerate==0.29.3
              
              # 启动模型服务
              python -m transformers.onnx --model=/model --feature=text-generation
              python -c "from transformers import pipeline; pipe = pipeline('text-generation', model='/model', device=0); print(pipe('Hello, how are you?', max_new_tokens=50))"
              
              # 保持容器运行
              sleep infinity
          env:
            - name: CUDA_VISIBLE_DEVICES
              value: "0"  # 指定使用第一个GPU
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            - name: LD_LIBRARY_PATH
              value: "/usr/local/cuda/lib64:/usr/local/nvidia/lib64:$LD_LIBRARY_PATH"
          resources:
            limits:
              nvidia.com/microgpu: 1  # 申请1个microgpu资源
          volumeMounts:
            - name: model-storage
              mountPath: /model
      volumes:
        - name: model-storage
          hostPath:
            path: /home/k8sadmin/Qwen1.5-0.5B-Chat
            type: Directory

---
# 验证服务（可选）
apiVersion: v1
kind: Service
metadata:
  name: qwen-test
spec:
  selector:
    app: qwen
  ports:
    - protocol: TCP
      port: 8888
      targetPort: 8888