# qwen-statefulset-local-direct.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: qwen-mini
spec:
  serviceName: "qwen-service"
  replicas: 1
  podManagementPolicy: OrderedReady
  selector:
    matchLabels:
      app: qwen
  template:
    metadata:
      labels:
        app: qwen
    spec:
      terminationGracePeriodSeconds: 600  # 延长终止宽限期，方便调试
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - qwen
              topologyKey: kubernetes.io/hostname
      containers:
        - name: text-generation
          image: ghcr.io/huggingface/text-generation-inference:1.4.1
          command: ["/bin/sh", "-c"]  # 改为shell模式执行
          args:
            - |
              # 诊断环境变量
              echo "===== 环境诊断信息 ====="
              echo "当前时间: $(date)"
              echo "CUDA环境:"
              ls -l /usr/local/cuda*
              echo "驱动库路径:"
              ls -l /usr/local/nvidia/lib64
              echo "环境变量:"
              printenv | sort
              
              # 检查CUDA设备
              echo "===== 设备检查 ====="
              ls -l /dev/nvidia*
              nvidia-smi
              
              # 检查PyTorch CUDA支持
              echo "===== PyTorch检查 ====="
              python -c "import torch; print(f'PyTorch版本: {torch.__version__}, CUDA可用: {torch.cuda.is_available()}')"
              
              # 检查bitsandbytes
              echo "===== bitsandbytes检查 ====="
              python -c "import bitsandbytes; print(f'bitsandbytes版本: {bitsandbytes.__version__}'); print(f'CUDA支持: {bitsandbytes.CUDASetup.get_instance().is_initialized}')"
              
              # 启动服务（保留原始命令）
              echo "===== 启动服务 ====="
              text-generation-launcher \
                --model-id /model \
                --num-shard 1 \
                --port 8000 \
                --quantize bitsandbytes
              
              # 保持容器运行（诊断模式）
              echo "===== 进入诊断模式 ====="
              sleep infinity
          env:
            - name: TRANSFORMERS_OFFLINE
              value: "1"
            - name: HF_HUB_OFFLINE
              value: "1"
            - name: NVIDIA_DISABLE_REQUIRE
              value: "1"
            # 新增关键环境变量
            - name: LD_LIBRARY_PATH
              value: "/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
            - name: CUDA_HOME
              value: "/usr/local/cuda"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            - name: FORCE_CUDA
              value: "1"  # 强制启用CUDA
          # 临时禁用探针，防止容器重启
          #readinessProbe:
          #  httpGet:
          #    path: /health
          #    port: 8000
          #  initialDelaySeconds: 120
          #  periodSeconds: 10
          #  failureThreshold: 30
          #livenessProbe:
          #  httpGet:
          #    path: /health
          #    port: 8000
          #  initialDelaySeconds: 180
          #  periodSeconds: 20
          #  failureThreshold: 10
          resources:
            limits:
              nvidia.com/microgpu: 1
            requests:
              nvidia.com/microgpu: 1  # 添加请求确保调度
          ports:
            - containerPort: 8000
          volumeMounts:
            - name: model-storage
              mountPath: /model
      volumes:
        - name: model-storage
          hostPath:
            path: /home/k8sadmin/Qwen1.5-0.5B-Chat
            type: Directory
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0

---
apiVersion: v1
kind: Service
metadata:
  name: qwen-service
spec:
  type: NodePort
  selector:
    app: qwen
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
      nodePort: 30080